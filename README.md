# GradientDescent
Implement the Stochastic Gradient Descent, Adam, and Adagrad, and compare their performance under the identical condition.

Stochastic Gradient Descent reduces the computational burden of the high-dimensional optimization compared to Gradient Descent.
While it is ture that Stochastic Gradient Descnet generally works well, it turns out not to be converged properly for the non-convex
loss functiosn. 

To compare the performance under the multiple conditons, I implemeted multiple optimization algorithms, and compared
them by plotting the graph of the mean error of each method.
